{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning in RUL Estimation\n",
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nominated-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries in python\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import importlib\n",
    "from itertools import repeat\n",
    "from scipy.stats import randint, expon, uniform\n",
    "\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import cv2\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Ignore tf err log\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "# random seed predictable\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Path settings\n",
    "current_dir = '.'#os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "## Dataset path\n",
    "train_FD001_path = current_dir +'/cmapss/train_FD001.csv'\n",
    "test_FD001_path = current_dir +'/cmapss/test_FD001.csv'\n",
    "RUL_FD001_path = current_dir+'/cmapss/RUL_FD001.txt'\n",
    "FD001_path = [train_FD001_path, test_FD001_path, RUL_FD001_path]\n",
    "\n",
    "train_FD002_path = current_dir +'/cmapss/train_FD002.csv'\n",
    "test_FD002_path = current_dir +'/cmapss/test_FD002.csv'\n",
    "RUL_FD002_path = current_dir +'/cmapss/RUL_FD002.txt'\n",
    "FD002_path = [train_FD002_path, test_FD002_path, RUL_FD002_path]\n",
    "\n",
    "train_FD003_path = current_dir +'/cmapss/train_FD003.csv'\n",
    "test_FD003_path = current_dir +'/cmapss/test_FD003.csv'\n",
    "RUL_FD003_path = current_dir +'/cmapss/RUL_FD003.txt'\n",
    "FD003_path = [train_FD003_path, test_FD003_path, RUL_FD003_path]\n",
    "\n",
    "train_FD004_path =current_dir +'/cmapss/train_FD004.csv'\n",
    "test_FD004_path = current_dir +'/cmapss/test_FD004.csv'\n",
    "RUL_FD004_path = current_dir +'/cmapss/RUL_FD004.txt'\n",
    "FD004_path = [train_FD004_path, test_FD004_path, RUL_FD004_path]\n",
    "\n",
    "## Assign columns name\n",
    "cols = ['unit_nr', 'cycles', 'os_1', 'os_2', 'os_3']\n",
    "cols += ['sensor_{0:02d}'.format(s + 1) for s in range(26)]\n",
    "col_rul = ['RUL_truth']\n",
    "\n",
    "## Read csv file to pandas dataframe\n",
    "FD_path = [\"none\", FD001_path, FD002_path, FD003_path, FD004_path]\n",
    "dp_str = [\"none\", \"FD001\", \"FD002\", \"FD003\", \"FD004\"]\n",
    "\n",
    "## temporary model path for NN\n",
    "model_path = current_dir +'/temp_net.h5'\n",
    "\n",
    "# Sensors not to be considered (those that do not disclose any pattern in their ts)\n",
    "sensor_drop = ['sensor_01', 'sensor_05', 'sensor_06', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']\n",
    "\n",
    "#start = time.time()\n",
    "\n",
    "# Architecture preferences\n",
    "dp = FD_path[1]\n",
    "subdataset = dp_str[1]\n",
    "sequence_length = 32\n",
    "thres_type = None\n",
    "thres_value = 50\n",
    "device = 'cpu'\n",
    "method = 'rps'\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 10\n",
    "epochs = 100\n",
    "batch = 700\n",
    "verbose = 2\n",
    "flatten = False\n",
    "visualize = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Plot File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on April , 2021\n",
    "@author:\n",
    "'''\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from pyts.image import RecurrencePlot\n",
    "from sklearn import preprocessing\n",
    "# from sklearn.decomposition import PCA\n",
    "# from pyts.approximation import SymbolicFourierApproximation\n",
    "\n",
    "\n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # for one id I put all the rows in a single matrix\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "\n",
    "    for start, stop in zip(range(0, num_elements - seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # For one id I put all the labels in a single matrix.\n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # I have to remove the first seq_length labels\n",
    "    # because for one id the first sequence of seq_length size have as target\n",
    "    # the last label (the previus ones are discarded).\n",
    "    # All the next id's sequences will have associated step by step one label as target.\n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "\n",
    "\n",
    "class input_gen(object):\n",
    "    '''\n",
    "    class for data preparation (rps generator)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, data_path_list, sequence_length, sensor_drop, piecewise_lin_ref=125, preproc=False, visualize=True):\n",
    "        '''\n",
    "        :param data_path_list: python list of four sub-dataset\n",
    "        :param sequence_length: legnth of sequence (sliced time series)\n",
    "        :param sensor_drop: sensors not to be considered\n",
    "        :param piecewise_lin_ref: max rul value (if real rul value is larger than piecewise_lin_ref,\n",
    "        then the rul value is piecewise_lin_ref)\n",
    "        :param preproc: preprocessing\n",
    "        '''\n",
    "        # self.__logger = logging.getLogger('data preparation for using it as the network input')\n",
    "        self.data_path_list = data_path_list\n",
    "        self.sequence_length = sequence_length\n",
    "        self.sensor_drop = sensor_drop\n",
    "        self.preproc = preproc\n",
    "        self.piecewise_lin_ref = piecewise_lin_ref\n",
    "        self.visualize = visualize\n",
    "\n",
    "\n",
    "        ## Assign columns name\n",
    "        cols = ['unit_nr', 'cycles', 'os_1', 'os_2', 'os_3']\n",
    "        cols += ['sensor_{0:02d}'.format(s + 1) for s in range(26)]\n",
    "        col_rul = ['RUL_truth']\n",
    "\n",
    "        train_FD = pd.read_csv(self.data_path_list[0], sep=' ', header=None,\n",
    "                               names=cols, index_col=False)\n",
    "        test_FD = pd.read_csv(self.data_path_list[1], sep=' ', header=None,\n",
    "                              names=cols, index_col=False)\n",
    "        RUL_FD = pd.read_csv(self.data_path_list[2], sep=' ', header=None,\n",
    "                             names=col_rul, index_col=False)\n",
    "\n",
    "        ## Calculate RUL and append to train data\n",
    "        # get the time of the last available measurement for each unit\n",
    "        mapper = {}\n",
    "        for unit_nr in train_FD['unit_nr'].unique():\n",
    "            mapper[unit_nr] = train_FD['cycles'].loc[train_FD['unit_nr'] == unit_nr].max()\n",
    "\n",
    "        # calculate RUL = time.max() - time_now for each unit\n",
    "        train_FD['RUL'] = train_FD['unit_nr'].apply(lambda nr: mapper[nr]) - train_FD['cycles']\n",
    "        # piecewise linear for RUL labels\n",
    "        train_FD['RUL'].loc[(train_FD['RUL'] > self.piecewise_lin_ref)] = self.piecewise_lin_ref\n",
    "\n",
    "        # Cut max RUL ground truth\n",
    "        RUL_FD['RUL_truth'].loc[(RUL_FD['RUL_truth'] > self.piecewise_lin_ref)] = self.piecewise_lin_ref\n",
    "\n",
    "        ## Excluse columns which only have NaN as value\n",
    "        cols_nan = train_FD.columns[train_FD.isna().any()].tolist()\n",
    "        cols_const = [col for col in train_FD.columns if len(train_FD[col].unique()) <= 2]\n",
    "\n",
    "        ## Drop exclusive columns\n",
    "        # train_FD = train_FD.drop(columns=cols_const + cols_nan)\n",
    "        # test_FD = test_FD.drop(columns=cols_const + cols_nan)\n",
    "\n",
    "        train_FD = train_FD.drop(columns=cols_const + cols_nan + sensor_drop)\n",
    "\n",
    "        test_FD = test_FD.drop(columns=cols_const + cols_nan + sensor_drop)\n",
    "\n",
    "\n",
    "        if self.preproc == True:\n",
    "            ## preprocessing(normailization for the neural networks)\n",
    "            min_max_scaler = preprocessing.MinMaxScaler()\n",
    "            # for the training set\n",
    "            # train_FD['cycles_norm'] = train_FD['cycles']\n",
    "            cols_normalize = train_FD.columns.difference(['unit_nr', 'cycles', 'os_1', 'os_2', 'RUL'])\n",
    "\n",
    "            norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_FD[cols_normalize]),\n",
    "                                         columns=cols_normalize,\n",
    "                                         index=train_FD.index)\n",
    "            join_df = train_FD[train_FD.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "            train_FD = join_df.reindex(columns=train_FD.columns)\n",
    "\n",
    "            # for the test set\n",
    "            # test_FD['cycles_norm'] = test_FD['cycles']\n",
    "            cols_normalize_test = test_FD.columns.difference(['unit_nr', 'cycles', 'os_1', 'os_2'])\n",
    "            # print (\"cols_normalize_test\", cols_normalize_test)\n",
    "            norm_test_df = pd.DataFrame(min_max_scaler.transform(test_FD[cols_normalize_test]), columns=cols_normalize_test,\n",
    "                                        index=test_FD.index)\n",
    "            test_join_df = test_FD[test_FD.columns.difference(cols_normalize_test)].join(norm_test_df)\n",
    "            test_FD = test_join_df.reindex(columns=test_FD.columns)\n",
    "            test_FD = test_FD.reset_index(drop=True)\n",
    "        else:\n",
    "            # print (\"No preprocessing\")\n",
    "            pass\n",
    "\n",
    "        # Specify the columns to be used\n",
    "        sequence_cols_train = train_FD.columns.difference(['unit_nr', 'cycles', 'os_1', 'os_2', 'RUL'])\n",
    "        sequence_cols_test = test_FD.columns.difference(['unit_nr', 'os_1', 'os_2', 'cycles'])\n",
    "\n",
    "\n",
    "\n",
    "        ## generator for the sequences\n",
    "        # transform each id of the train dataset in a sequence\n",
    "        seq_gen = (list(gen_sequence(train_FD[train_FD['unit_nr'] == id], self.sequence_length, sequence_cols_train))\n",
    "                   for id in train_FD['unit_nr'].unique())\n",
    "\n",
    "        # generate sequences and convert to numpy array in training set\n",
    "        seq_array_train = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "        self.seq_array_train = seq_array_train.transpose(0, 2, 1) # shape = (samples, sensors, sequences)\n",
    "        # print(\"seq_array_train.shape\", self.seq_array_train.shape)\n",
    "\n",
    "        # generate label of training samples\n",
    "        label_gen = [gen_labels(train_FD[train_FD['unit_nr'] == id], self.sequence_length, ['RUL'])\n",
    "                     for id in train_FD['unit_nr'].unique()]\n",
    "        self.label_array_train = np.concatenate(label_gen).astype(np.float32)\n",
    "\n",
    "        # generate sequences and convert to numpy array in test set (only the last sequence for each engine in test set)\n",
    "        seq_array_test_last = [test_FD[test_FD['unit_nr'] == id][sequence_cols_test].values[-self.sequence_length:]\n",
    "                               for id in test_FD['unit_nr'].unique() if\n",
    "                               len(test_FD[test_FD['unit_nr'] == id]) >= self.sequence_length]\n",
    "\n",
    "        seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "        self.seq_array_test_last = seq_array_test_last.transpose(0, 2, 1) # shape = (samples, sensors, sequences)\n",
    "        # print(\"seq_array_test_last.shape\", self.seq_array_test_last.shape)\n",
    "\n",
    "        # generate label of test samples\n",
    "        y_mask = [len(test_FD[test_FD['unit_nr'] == id]) >= self.sequence_length for id in test_FD['unit_nr'].unique()]\n",
    "        label_array_test_last = RUL_FD['RUL_truth'][y_mask].values\n",
    "        self.label_array_test = label_array_test_last.reshape(label_array_test_last.shape[0], 1).astype(np.float32)\n",
    "\n",
    "\n",
    "        ## Visualize Run-2-failure TS of the first engine in the training set.(Please deactivate after understanding)\n",
    "        if self.visualize == True:\n",
    "            # R2F TS of the first engine\n",
    "            pd.DataFrame(train_FD[train_FD['unit_nr'] == 1][sequence_cols_train].values,\n",
    "                             columns=sequence_cols_train).plot(subplots=True, figsize=(15, 15))\n",
    "\n",
    "            # The last sequences sliced from each TS (of the first engine)\n",
    "            prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "            colors = prop_cycle.by_key()['color']\n",
    "            colors = colors + colors + colors\n",
    "\n",
    "            seq_gen = (\n",
    "            list(gen_sequence(train_FD[train_FD['unit_nr'] == id], self.sequence_length, sequence_cols_train))\n",
    "            for id in train_FD['unit_nr'].unique())\n",
    "\n",
    "            seq_list_engine = list(seq_gen)\n",
    "            seq_engine_1_array = np.asarray(seq_list_engine[0])\n",
    "\n",
    "            last_seq_engine_1_array = seq_engine_1_array[-1, :, :]\n",
    "            fig_ts = plt.figure(figsize=(15, 15))\n",
    "            for s in range(last_seq_engine_1_array.shape[1]):\n",
    "                seq_s = last_seq_engine_1_array[:, s]\n",
    "                # plt.subplot(last_seq_engine_1_array.shape[1],(s//4) + 1, (s%4)+1)\n",
    "                plt.subplot(4, 4, s + 1)\n",
    "                plt.plot(seq_s, \"y\", label=sequence_cols_train[s], color=colors[s])\n",
    "                plt.legend()\n",
    "\n",
    "            plt.xlabel(\"time(cycles)\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def rps(self, thres_type=None, thres_percentage=50, flatten=False, visualize=True):\n",
    "        '''\n",
    "        generate RPs from sequences\n",
    "        :param thres_type:  ‘point’, ‘distance’ or None (default = None)\n",
    "        :param thres_percentage:\n",
    "        :param flatten:\n",
    "        :param visualize: visualize generated RPs (first training sample)\n",
    "        :return: PRs (samples for NNs and their label)\n",
    "        '''\n",
    "\n",
    "        # Recurrence plot transformation for training samples\n",
    "        rp_train = RecurrencePlot(threshold=thres_type, percentage=thres_percentage,flatten=flatten)\n",
    "\n",
    "        rp_list = []\n",
    "        for idx in range(self.seq_array_train.shape[0]):\n",
    "            temp_mts = self.seq_array_train[idx]\n",
    "            # print (temp_mts.shape)\n",
    "            X_rp_temp = rp_train.fit_transform(temp_mts)\n",
    "            # print (X_rp_temp.shape)\n",
    "            rp_list.append(X_rp_temp)\n",
    "\n",
    "        rp_train_samples = np.stack(rp_list, axis=0)\n",
    "\n",
    "        # Recurrence plot transformation for test samples\n",
    "        rp_test = RecurrencePlot(threshold=thres_type, percentage=thres_percentage, flatten=flatten)\n",
    "        rp_list = []\n",
    "        for idx in range(self.seq_array_test_last.shape[0]):\n",
    "            temp_mts = self.seq_array_test_last[idx]\n",
    "            # print (temp_mts.shape)\n",
    "            X_rp_temp = rp_test.fit_transform(temp_mts)\n",
    "            # print (X_rp_temp.shape)\n",
    "            rp_list.append(X_rp_temp)\n",
    "        rp_test_samples = np.stack(rp_list, axis=0)\n",
    "\n",
    "        label_array_train = self.label_array_train\n",
    "        label_array_test = self.label_array_test\n",
    "\n",
    "        # Visualize RPs of the last sequences sliced from each TS (of the first engine)\n",
    "        if visualize == True:\n",
    "            X_rp = rp_train_samples[-1]\n",
    "            plt.figure(figsize=(15, 15))\n",
    "            for s in range(len(X_rp)):\n",
    "                # plt.subplot(last_seq_engine_1_array.shape[1],(s//4) + 1, (s%4)+1)\n",
    "                plt.subplot(4, 4, s + 1)\n",
    "                if flatten == True:\n",
    "                    img = np.atleast_2d(X_rp[s])\n",
    "                    plt.imshow(img, extent=(0, img.shape[1], 0, round(img.shape[1]/9)))\n",
    "                else:\n",
    "                    plt.imshow(X_rp[s], origin='lower')\n",
    "                # plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        return  rp_train_samples, label_array_train, rp_test_samples, label_array_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "recovered-despite",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_class = input_gen(data_path_list=dp, sequence_length=sequence_length, sensor_drop= sensor_drop, visualize=visualize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == 'rps':\n",
    "    train_samples, label_array_train, test_samples, label_array_test = data_class.rps(\n",
    "        thres_type=thres_type,\n",
    "        thres_percentage=thres_value,\n",
    "        flatten=flatten,\n",
    "        visualize=visualize)\n",
    "\n",
    "elif method == 'jrp': # please implement any method if needed\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17431"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_format(X_rp):\n",
    "    '''\n",
    "    Takes 14 sensors array (32x32) and reshapes into 1 array(128x128)\n",
    "    '''\n",
    "    # 1- Create two dummy zero arrays with np.zeros(32,32)\n",
    "    temp = np.zeros((64,32))\n",
    "    # 2- Concatenate all 16 arrays\n",
    "    for i in range(len(X_rp)):\n",
    "        x = X_rp[i]\n",
    "        temp = np.concatenate((temp,x), axis=0)\n",
    "\n",
    "    # 3- Reshape the concatenated arrays to (128,128)\n",
    "    new_train0 = temp.reshape(128,128)\n",
    "    # 4- Extract RGB channels with cv2.split(img)\n",
    "    ## To make a figure without the frame\n",
    "    my_dpi = 96\n",
    "    array_size = 128\n",
    "    fig = plt.figure(figsize=(array_size/my_dpi, array_size/my_dpi), dpi=my_dpi, frameon=False)\n",
    "    ## To make the content fill the whole figure\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    ## Drawing image\n",
    "    ax.imshow(new_train0)\n",
    "    fig.savefig('img1.png', dpi = my_dpi)\n",
    "    img = cv2.imread('img1.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return np.asarray(cv2.split(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start,end):\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "\n",
    "def format_samples(train_samples, test_samples):\n",
    "    '''\n",
    "    Format training and test sets (n_samples, image_size, channels)\n",
    "    '''\n",
    "    start = time.time()\n",
    "    resh_train = np.asarray([change_format(rp) for rp in train_samples])\n",
    "    end = time.time()\n",
    "    print(\"Reshape train time: \")\n",
    "    timer(start,end)\n",
    "    start = time.time()\n",
    "    resh_test = np.asarray([change_format(rp) for rp in test_samples])\n",
    "    end = time.time()\n",
    "    print(\"Reshape test time: \")\n",
    "    timer(start,end)\n",
    "    return resh_train, resh_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = format_samples(train_samples, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-spectacular",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"train_samples.shape: \", train.shape) # shape = (samples, channels, height, width)\n",
    "print (\"label_array_train.shape: \", label_array_train.shape) # shape = (samples, label)\n",
    "print (\"test_samples.shape: \", test.shape) # shape = (samples, channels, height, width)\n",
    "print (\"label_array_test.shape: \", label_array_test.shape) # shape = (samples, ground truth)\n",
    "\n",
    "train = np.transpose(train, (0,2,3,1))\n",
    "test = np.transpose(test, (0,2,3,1))\n",
    "\n",
    "print (\"train_samples.shape: \", train.shape) # shape = (samples, height, width, channels)\n",
    "print (\"label_array_train.shape: \", label_array_train.shape) # shape = (samples, label)\n",
    "print (\"test_samples.shape: \", test.shape) # shape = (samples, height, width, channels)\n",
    "print (\"label_array_test.shape: \", label_array_test.shape) # shape = (samples, ground truth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import logging as log\n",
    "import sys\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import importlib\n",
    "from scipy.stats import randint, expon, uniform\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from math import sqrt\n",
    "# import keras\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# import keras.backend as K\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Embedding\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, LSTM, TimeDistributed\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "\n",
    "def gen_net():\n",
    "    '''\n",
    "    TODO: Generate and evaluate any CNN instead of MLPs\n",
    "    :param vec_len:\n",
    "    :param num_hidden1:\n",
    "    :param num_hidden2:\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "\n",
    "    # input_shape deve essere 30x30, cioé la grandezza dell'immagine di training che abbiamo\n",
    "    # il parametro include_top parametrizzato a false indica che non verrá caricato l'ultimo layer\n",
    "    vgg = VGG16(input_shape=(128,128,3), weights = 'imagenet', include_top = False)\n",
    "\n",
    "    # Passaggio fondamentale é non trainare i pesi esistenti in vgg\n",
    "    for layer in vgg.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    \n",
    "    base_outputs = vgg.output\n",
    "    final_outputs = Dense(1)(base_outputs)\n",
    "    new_model = Model(inputs = vgg.input, outputs = final_outputs) \n",
    "    new_model.summary()\n",
    "\n",
    "    return new_model\n",
    "\n",
    "\n",
    "# def gen_net(vec_len, num_hidden1, num_hidden2 ):\n",
    "#     '''\n",
    "#     TODO: Generate and evaluate any CNN instead of MLPs\n",
    "#     :param vec_len:\n",
    "#     :param num_hidden1:\n",
    "#     :param num_hidden2:\n",
    "#     :return:\n",
    "#     '''\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(num_hidden1, activation='relu', input_shape=(vec_len,)))\n",
    "#     model.add(Dense(num_hidden2, activation='relu'))\n",
    "#     model.add(Dense(1))\n",
    "\n",
    "#     return model\n",
    "\n",
    "class network_fit(object):\n",
    "    '''\n",
    "    class for network\n",
    "    '''\n",
    "\n",
    "    def __init__(self, train_samples, label_array_train, test_samples, label_array_test,\n",
    "                 model_path, n_hidden1 =100, n_hidden2 =10, verbose=2):\n",
    "        '''\n",
    "        Constructor\n",
    "        Generate a NN and train\n",
    "        @param none\n",
    "        '''\n",
    "        # self.__logger = logging.getLogger('data preparation for using it as the network input')\n",
    "        self.train_samples = train_samples\n",
    "        self.label_array_train = label_array_train\n",
    "        self.test_samples = test_samples\n",
    "        self.label_array_test = label_array_test\n",
    "        self.n_hidden1 = n_hidden1\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.model_path = model_path\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # self.mlps = gen_net(self.train_samples.shape[1], self.n_hidden1, self.n_hidden2)\n",
    "        self.mlps = gen_net()\n",
    "\n",
    "\n",
    "\n",
    "    def train_net(self, epochs = 1000, batch_size= 700, lr= 1e-05, plotting=True):\n",
    "        '''\n",
    "        specify the optimizers and train the network\n",
    "        :param epochs:\n",
    "        :param batch_size:\n",
    "        :param lr:\n",
    "        :return:\n",
    "        '''\n",
    "        print(\"Initializing network...\")\n",
    "        # compile the model\n",
    "        rp = optimizers.RMSprop(learning_rate=lr, rho=0.9, centered=True)\n",
    "        adm = optimizers.Adam(learning_rate=lr, epsilon=1)\n",
    "        sgd_m = optimizers.SGD(learning_rate=lr)\n",
    "\n",
    "        keras_rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "        self.mlps.compile(loss='mean_squared_error', optimizer=sgd_m, metrics=[keras_rmse, 'mae'])\n",
    "\n",
    "        print(self.mlps.summary())\n",
    "\n",
    "        # Train the model\n",
    "        history = self.mlps.fit(self.train_samples, self.label_array_train, epochs=epochs, batch_size=batch_size,\n",
    "                                validation_split=0.2, verbose=self.verbose,\n",
    "                                callbacks=[\n",
    "                               EarlyStopping(monitor='val_root_mean_squared_error', min_delta=0, patience=50, verbose=self.verbose, mode='min'),\n",
    "                               ModelCheckpoint(self.model_path, monitor='val_root_mean_squared_error', save_best_only=True, mode='min',\n",
    "                                               verbose=self.verbose)])\n",
    "\n",
    "        val_rmse_k = history.history['val_root_mean_squared_error']\n",
    "        val_rmse_min = min(val_rmse_k)\n",
    "        min_val_rmse_idx = val_rmse_k.index(min(val_rmse_k))\n",
    "        stop_epoch = min_val_rmse_idx +1\n",
    "        val_rmse_min = round(val_rmse_min, 4)\n",
    "        print (\"val_rmse_min: \", val_rmse_min)\n",
    "\n",
    "        trained_net = self.mlps\n",
    "\n",
    "        ## Plot training & validation loss about epochs\n",
    "        if plotting == True:\n",
    "            # summarize history for Loss\n",
    "            fig_acc = plt.figure(figsize=(10, 10))\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.ylim(0, 2000)\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        return trained_net\n",
    "\n",
    "\n",
    "\n",
    "    def test_net(self, trained_net=None, best_model=True, plotting=True):\n",
    "        '''\n",
    "        Evalute the trained network on test set\n",
    "        :param trained_net:\n",
    "        :param best_model:\n",
    "        :param plotting:\n",
    "        :return:\n",
    "        '''\n",
    "        # Load the trained model\n",
    "        if best_model:\n",
    "            estimator = load_model(self.model_path)\n",
    "        else:\n",
    "            estimator = load_model(trained_net)\n",
    "\n",
    "        # predict the RUL\n",
    "        y_pred_test = estimator.predict(self.test_samples)\n",
    "        y_true_test = self.label_array_test # ground truth of test samples\n",
    "\n",
    "        pd.set_option('display.max_rows', 1000)\n",
    "        test_print = pd.DataFrame()\n",
    "        test_print['y_pred'] = y_pred_test.flatten()\n",
    "        test_print['y_truth'] = y_true_test.flatten()\n",
    "        test_print['diff'] = abs(y_pred_test.flatten() - y_true_test.flatten())\n",
    "        test_print['diff(ratio)'] = abs(y_pred_test.flatten() - y_true_test.flatten()) / y_true_test.flatten()\n",
    "        test_print['diff(%)'] = (abs(y_pred_test.flatten() - y_true_test.flatten()) / y_true_test.flatten()) * 100\n",
    "\n",
    "        y_predicted = test_print['y_pred']\n",
    "        y_actual = test_print['y_truth']\n",
    "        rms = sqrt(mean_squared_error(y_actual, y_predicted)) # RMSE metric\n",
    "        test_print['rmse'] = rms\n",
    "        print(test_print)\n",
    "\n",
    "\n",
    "        # Score metric\n",
    "        h_array = y_predicted - y_actual\n",
    "        s_array = np.zeros(len(h_array))\n",
    "        for j, h_j in enumerate(h_array):\n",
    "            if h_j < 0:\n",
    "                s_array[j] = math.exp(-(h_j / 13)) - 1\n",
    "\n",
    "            else:\n",
    "                s_array[j] = math.exp(h_j / 10) - 1\n",
    "        score = np.sum(s_array)\n",
    "\n",
    "        # Plot the results of RUL prediction\n",
    "        if plotting == True:\n",
    "            fig_verify = plt.figure(figsize=(12, 6))\n",
    "            plt.plot(y_pred_test, color=\"blue\")\n",
    "            plt.plot(y_true_test, color=\"green\")\n",
    "            plt.title('prediction')\n",
    "            plt.ylabel('value')\n",
    "            plt.xlabel('row')\n",
    "            plt.legend(['predicted', 'actual data'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "        return rms, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-courtesy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo la cnn tenendo conto che non ho concatenato gli input e che quindi non serviranno:\n",
    "# - Hidden\n",
    "# - Deve essere modificato nel gen_net lo shape da passare al costruttore perché é 30x30\n",
    "mlps_net = network_fit(train, label_array_train, test, label_array_test,\n",
    "                       model_path = model_path, n_hidden1=n_hidden1, n_hidden2=n_hidden2, verbose=verbose)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trained_net = mlps_net.train_net(epochs=epochs, batch_size=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms, score = mlps_net.test_net(trained_net)\n",
    "\n",
    "\n",
    "print(subdataset + \" test RMSE: \", rms)\n",
    "print(subdataset + \" test Score: \", score)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Computing time: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning and Fine Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of loading the inception v3 model\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "# load model\n",
    "new_input = tf.keras.Input(30,30,3)\n",
    "model = InceptionV3()\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
